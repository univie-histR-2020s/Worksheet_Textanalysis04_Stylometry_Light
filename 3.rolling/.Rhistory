nrow(table)
table = table[table$freq >= quantile(table$freq)[2],]
nrow(table)
table = table[table$freq >= quantile(table$freq)[4]*1.5,]
nrow(table)
table[1:5]
table = sub[order(-sub$freq),]
table = table[table$freq >= quantile(table$freq)[2],]
table = table[table$freq >= quantile(table$freq)[4]*3,]
table
table = sub[order(-sub$freq),]
table = table[table$freq >= quantile(table$freq)[4]*3,]
listReport <- paste0(table$val," (",table$freq,")")
table
knit_with_parameters('~/Documents/_ResearchData/MESA_Conferences/R_Markdown/2016-12-01-mesa.Rmd')
featType    = c("w", "c")
featType[2]
library(stylo)
print("")
print(paste("==================\n","Generating:", i, "for", ii, "features (", featType, featLen, ")\n=================="))
for (i in classifiers){
for (ii in mfwVector){
for (iii in featType){
if (iii == "w"){
featType = "w"
featLen  = "1"
incr = 1
} else {
featType = "c"
featLen  = "2"
incr = 2
}
print(paste("==================\\n","Generating:", i, "for", ii, "features (", featType, featLen, ")\\n=================="))
rolling.classify(write.pdf.file = TRUE, corpus.lang="Other",
analyzed.features = featType, ngram.size = featLen,
classification.method = i, mfw=ii+incr,
use.existing.freq.tables = TRUE,
training.set.sampling = "normal.sampling",
slice.size = 5000, slice.overlap = 4500,
milestone.labels = milestone_labels)
}
}
}
classifiers = c("svm", "delta", "nsc")
mfwVector   = c(100, 300, 500, 1000)
featType    = c("w", "c")
for (i in classifiers){
for (ii in mfwVector){
for (iii in featType){
if (iii == "w"){
featType = "w"
featLen  = "1"
incr = 1
} else {
featType = "c"
featLen  = "2"
incr = 2
}
print(paste("==================\\n","Generating:", i, "for", ii, "features (", featType, featLen, ")\\n=================="))
rolling.classify(write.pdf.file = TRUE, corpus.lang="Other",
analyzed.features = featType, ngram.size = featLen,
classification.method = i, mfw=ii+incr,
use.existing.freq.tables = TRUE,
training.set.sampling = "normal.sampling",
slice.size = 5000, slice.overlap = 4500,
milestone.labels = milestone_labels)
}
}
}
paste("==================\\n","Generating:", i, "for", ii, "features (", featType, featLen, ")\\n==================")
print("==================\\n","Generating:", i, "for", ii, "features (", featType, featLen, ")\\n==================")
rep=paste("==================\\n","Generating:", i, "for", ii, "features (", featType, featLen, ")\\n=================="))
rep=paste("==================\\n","Generating:", i, "for", ii, "features (", featType, featLen, ")\\n==================")
print(per)
print(rep)
rep=paste("==================\n","Generating:", i, "for", ii, "features (", featType, featLen, ")\n==================")
print(rep)
rep=paste("==================Generating:", i, "for", ii, "features (", featType, featLen, ")==================")
print(rep)
rep=paste("==================Generating:", i, "for", ii, "features (", featType, featLen, ")==================")
cat(rep)
rep=paste("==================\nGenerating:", i, "for", ii, "features (", featType, featLen, ")==================")
cat(rep)
rep=paste("==================Generating:", i, "for", ii, "features (", featType, featLen, ")==================")
cat(rep)
library(readr)
library(readr)
networkFile = "/Users/romanov/Dropbox/KITAB Development Work/networkBooks_noOutliers_Type_directed.csv"
dispatch.articles <- read.csv(file=networkFile,
header=F, sep="\t", quote="",
stringsAsFactors=FALSE)
networkData <- read.csv(file=networkFile,
header=F, sep="\t", quote="",
stringsAsFactors=FALSE)
networkData <- read.csv(file=networkFile,
header=F, sep="\t", quote="",
stringsAsFactors=FALSE)
View(networkData)
networkData <- read.csv(file=networkFile,
header=T, sep="\t", quote="",
stringsAsFactors=FALSE)
summary(networkData$Weight)
((35-5)+(35-10)+(35-15))/3
((35-5)+(35-10)+(35-15)+(35-20))/3
((35-5)+(35-10)+(35-15)+(35-20))/4
((35-5)+(35-10)+(35-20))/3
((35-5)+(35-5)+(35-20))/3
install.packages("stylo")
library(stylo)
setwd(".")
getwd()
setwd("/Users/romanov/Dropbox/6.Teaching/_2017_2/_20180123_R_Stylometry/100_english_novels")
stylo()
stylo()
stylo()
detectCores()
library(stylo)
setwd("/Users/romanov/Dropbox/6.Teaching/_2017_2/_20180123_R_Stylometry/2.classify")
classify()
classify(
corpus.format="plain",
corpus.lang="English.all",
analyzed.features="w",
ngram.size=1,
mfw.min=100,
mfw.max=500,
mfw.incr=100,
start.at=1,
culling.min=0,
culling.max=50,
culling.incr=10,
mfw.list.cutoff=5000,
delete.pronouns=FALSE,
preserve.case=FALSE,
encoding="native.enc",
use.existing.freq.tables=FALSE,
use.existing.wordlist=FALSE,
classification.method="delta",
culling.of.all.samples=TRUE,
z.scores.of.all.samples=FALSE,
reference.wordlist.of.all.samples=FALSE,
distance.measure="delta",
svm.kernel="linear",
svm.degree=3,
svm.coef0=0,
svm.cost=1,
k.value=1,
l.value=0,
sampling="no.sampling",
sample.size=10000,
number.of.samples=1,
final.ranking.of.candidates=TRUE,
how.many.correct.attributions=TRUE,
number.of.candidates=3,
save.distance.tables=FALSE,
save.analyzed.features=FALSE,
save.analyzed.freqs=FALSE,
)
classify(
gui=FALSE,
corpus.format="plain",
corpus.lang="English.all",
analyzed.features="w",
ngram.size=1,
mfw.min=100,
mfw.max=500,
mfw.incr=100,
start.at=1,
culling.min=0,
culling.max=50,
culling.incr=10,
mfw.list.cutoff=5000,
delete.pronouns=FALSE,
preserve.case=FALSE,
encoding="native.enc",
use.existing.freq.tables=FALSE,
use.existing.wordlist=FALSE,
classification.method="delta",
culling.of.all.samples=TRUE,
z.scores.of.all.samples=FALSE,
reference.wordlist.of.all.samples=FALSE,
distance.measure="delta",
svm.kernel="linear",
svm.degree=3,
svm.coef0=0,
svm.cost=1,
k.value=1,
l.value=0,
sampling="no.sampling",
sample.size=10000,
number.of.samples=1,
final.ranking.of.candidates=TRUE,
how.many.correct.attributions=TRUE,
number.of.candidates=3,
save.distance.tables=FALSE,
save.analyzed.features=FALSE,
save.analyzed.freqs=FALSE
)
classify(
gui=FALSE,
corpus.format="plain",
corpus.lang="English.all",
analyzed.features="w",
ngram.size=1,
mfw.min=100,
mfw.max=500,
mfw.incr=100,
start.at=1,
culling.min=0,
culling.max=50,
culling.incr=10,
mfw.list.cutoff=5000,
delete.pronouns=FALSE,
preserve.case=FALSE,
encoding="native.enc",
use.existing.freq.tables=FALSE,
use.existing.wordlist=FALSE,
classification.method="delta",
culling.of.all.samples=TRUE,
z.scores.of.all.samples=FALSE,
reference.wordlist.of.all.samples=FALSE,
distance.measure="delta",
svm.kernel="linear",
svm.degree=3,
svm.coef0=0,
svm.cost=1,
k.value=1,
l.value=0,
sampling="no.sampling",
sample.size=10000,
number.of.samples=1,
final.ranking.of.candidates=TRUE,
how.many.correct.attributions=TRUE,
number.of.candidates=3,
save.distance.tables=FALSE,
save.analyzed.features=FALSE,
save.analyzed.freqs=FALSE
)
getwd()
setwd("/Users/romanov/Dropbox/6.Teaching/_2017_2/_20180123_R_Stylometry/2.classify/")
getwd()
classify(
gui=FALSE,
corpus.format="plain",
corpus.lang="English.all",
analyzed.features="w",
ngram.size=1,
mfw.min=100,
mfw.max=500,
mfw.incr=100,
start.at=1,
culling.min=0,
culling.max=50,
culling.incr=10,
mfw.list.cutoff=5000,
delete.pronouns=FALSE,
preserve.case=FALSE,
encoding="native.enc",
use.existing.freq.tables=FALSE,
use.existing.wordlist=FALSE,
classification.method="delta",
culling.of.all.samples=TRUE,
z.scores.of.all.samples=FALSE,
reference.wordlist.of.all.samples=FALSE,
distance.measure="delta",
svm.kernel="linear",
svm.degree=3,
svm.coef0=0,
svm.cost=1,
k.value=1,
l.value=0,
sampling="no.sampling",
sample.size=10000,
number.of.samples=1,
final.ranking.of.candidates=TRUE,
how.many.correct.attributions=TRUE,
number.of.candidates=3,
save.distance.tables=FALSE,
save.analyzed.features=FALSE,
save.analyzed.freqs=FALSE
)
setwd("/Users/romanov/Dropbox/6.Teaching/_2017_2/_20180123_R_Stylometry/1.standard/")
stylo()
stylo(
giu=FALSE,
corpus.format = "plain",
corpus.lang = "English",
analyzed.features = "w",
ngram.size = 1,
preserve.case = FALSE,
encoding = "UTF-8",
mfw.min = 100,
mfw.max = 500,
mfw.incr = 100,
start.at = 1,
culling.min = 10,
culling.max = 60,
culling.incr = 20,
mfw.list.cutoff = 5000,
delete.pronouns = FALSE,
use.existing.freq.tables = FALSE,
use.existing.wordlist = FALSE,
use.custom.list.of.files = FALSE,
analysis.type = "BCT",
consensus.strength = 0.5,
sampling = "no.sampling",
sample.size = 10000,
number.of.samples = 1,
display.on.screen = TRUE,
write.pdf.file = TRUE,
write.jpg.file = TRUE,
write.svg.file = TRUE,
write.png.file = TRUE,
plot.custom.height = 15,
plot.custom.width = 15,
plot.font.size = 10,
plot.line.thickness = 2,
text.id.on.graphs = "both",
colors.on.graphs = "colors",
titles.on.graphs = TRUE,
label.offset = 3,
add.to.margins = 2,
dendrogram.layout.horizontal = TRUE,
pca.visual.flavour = "classic",
save.distance.tables = FALSE,
save.analyzed.features = FALSE,
save.analyzed.freqs = FALSE,
dump.samples = FALSE
)
stylo(
gui=FALSE,
corpus.format = "plain",
corpus.lang = "English",
analyzed.features = "w",
ngram.size = 1,
preserve.case = FALSE,
encoding = "UTF-8",
mfw.min = 100,
mfw.max = 500,
mfw.incr = 100,
start.at = 1,
culling.min = 10,
culling.max = 60,
culling.incr = 20,
mfw.list.cutoff = 5000,
delete.pronouns = FALSE,
use.existing.freq.tables = FALSE,
use.existing.wordlist = FALSE,
use.custom.list.of.files = FALSE,
analysis.type = "BCT",
consensus.strength = 0.5,
sampling = "no.sampling",
sample.size = 10000,
number.of.samples = 1,
display.on.screen = TRUE,
write.pdf.file = TRUE,
write.jpg.file = TRUE,
write.svg.file = TRUE,
write.png.file = TRUE,
plot.custom.height = 15,
plot.custom.width = 15,
plot.font.size = 10,
plot.line.thickness = 2,
text.id.on.graphs = "both",
colors.on.graphs = "colors",
titles.on.graphs = TRUE,
label.offset = 3,
add.to.margins = 2,
dendrogram.layout.horizontal = TRUE,
pca.visual.flavour = "classic",
save.distance.tables = FALSE,
save.analyzed.features = FALSE,
save.analyzed.freqs = FALSE,
dump.samples = FALSE
)
dirname(parent.frame(2)$ofile)
getSrcDirectory()[1]
dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(".")
getwd()
setwd("/Users/romanov/Dropbox/6.Teaching/_2017_2/2017W_R_For_HistoricalResearch/SNA/ch06/scripts/")
getwd()
setwd(".")
getwd()
setwd(".")
getwd()
dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(rstudioapi)
dirname(getActiveDocumentContext()$path)
getwd()
setwd(dirname(getActiveDocumentContext()$path))
library(rstudioapi) # make sure you have it installed
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path ))
print( getwd() )
library(rstudioapi) # make sure you have it installed
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path))
print(getwd())
library(rstudioapi) # make sure you have it installed
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path))
print(getwd())
stylo(
gui=FALSE,
corpus.format = "plain",
corpus.lang = "English",
analyzed.features = "w",
ngram.size = 1,
preserve.case = FALSE,
encoding = "UTF-8",
mfw.min = 100,
mfw.max = 500,
mfw.incr = 100,
start.at = 1,
culling.min = 10,
culling.max = 60,
culling.incr = 20,
mfw.list.cutoff = 5000,
delete.pronouns = FALSE,
use.existing.freq.tables = FALSE,
use.existing.wordlist = FALSE,
use.custom.list.of.files = FALSE,
analysis.type = "BCT",
consensus.strength = 0.5,
sampling = "no.sampling",
sample.size = 10000,
number.of.samples = 1,
display.on.screen = TRUE,
write.pdf.file = TRUE,
write.jpg.file = TRUE,
write.svg.file = TRUE,
write.png.file = TRUE,
plot.custom.height = 15,
plot.custom.width = 15,
plot.font.size = 10,
plot.line.thickness = 2,
text.id.on.graphs = "both",
colors.on.graphs = "colors",
titles.on.graphs = TRUE,
label.offset = 3,
add.to.margins = 2,
dendrogram.layout.horizontal = TRUE,
pca.visual.flavour = "classic",
save.distance.tables = FALSE,
save.analyzed.features = FALSE,
save.analyzed.freqs = FALSE,
dump.samples = FALSE
)
library(rstudioapi) # make sure you have it installed
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path))
print(getwd())
classify(
gui=FALSE,
corpus.format="plain",
corpus.lang="English.all",
analyzed.features="w",
ngram.size=1,
mfw.min=100,
mfw.max=500,
mfw.incr=100,
start.at=1,
culling.min=0,
culling.max=50,
culling.incr=10,
mfw.list.cutoff=5000,
delete.pronouns=FALSE,
preserve.case=FALSE,
encoding="native.enc",
use.existing.freq.tables=FALSE,
use.existing.wordlist=FALSE,
classification.method="delta",
culling.of.all.samples=TRUE,
z.scores.of.all.samples=FALSE,
reference.wordlist.of.all.samples=FALSE,
distance.measure="delta",
svm.kernel="linear",
svm.degree=3,
svm.coef0=0,
svm.cost=1,
k.value=1,
l.value=0,
sampling="no.sampling",
sample.size=10000,
number.of.samples=1,
final.ranking.of.candidates=TRUE,
how.many.correct.attributions=TRUE,
number.of.candidates=3,
save.distance.tables=FALSE,
save.analyzed.features=FALSE,
save.analyzed.freqs=FALSE
)
rolling.classify(write.png.file = TRUE, classification.method = "svm", mfw=100, training.set.sampling = "normal.sampling", slice.size = 5000, slice.overlap = 4500)
library(rstudioapi)
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path))
print(getwd())
library(stylo)
rolling.classify(write.png.file = TRUE, classification.method = "svm", mfw=100, training.set.sampling = "normal.sampling", slice.size = 5000, slice.overlap = 4500)
rolling.classify(write.png.file = TRUE,
classification.method = "nsc",
mfw=50,
training.set.sampling = "normal.sampling",
slice.size = 5000, slice.overlap = 4500)
rolling.classify(write.png.file = TRUE,
classification.method = "delta",
mfw=1000)
rolling.classify(write.png.file = TRUE,
classification.method = "delta",
mfw=500)
